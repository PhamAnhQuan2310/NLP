import streamlit as st
import string
import requests
import pandas as pd
import numpy as np
from bs4 import BeautifulSoup

import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer

import spacy
import fasttext.util
from gensim.downloader import load as gensim_load

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
from sklearn.metrics.pairwise import cosine_similarity
import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
import unicodedata
from sklearn.pipeline import make_pipeline

nltk.download('stopwords')
nltk.download('wordnet')
nlp = spacy.load("en_core_web_sm")

st.set_page_config(page_title="NLP Processing App", layout="centered")
st.markdown("""
    <style>
        .main {background-color: #f9f9f9;}
        .block-container {
            max-width: 900px;
            padding-top: 2rem;
            padding-bottom: 2rem;
        }
        h1, h2, h3 {
            color: #274060;
            font-family: 'Segoe UI', sans-serif;
        }
        .stTextInput > div > input {
            background-color: #ffffff;
            color: black;
        }
    </style>
""", unsafe_allow_html=True)

st.title("·ª®ng d·ª•ng X·ª≠ L√Ω Ng√¥n Ng·ªØ T·ª± Nhi√™n")
st.caption("Bao g·ªìm c√°c b∆∞·ªõc: TƒÉng c∆∞·ªùng d·ªØ li·ªáu, Ti·ªÅn x·ª≠ l√Ω, Vector h√≥a & Nh√∫ng vƒÉn b·∫£n")

# -----------------------------
# Nh·∫≠p d·ªØ li·ªáu
# -----------------------------
st.header("Nh·∫≠p d·ªØ li·ªáu")
input_method = st.selectbox("Ch·ªçn ngu·ªìn d·ªØ li·ªáu", ["Nh·∫≠p tay", "T·∫£i dataset CSV", "C√†o t·ª´ web"])
data = []

if input_method == "Nh·∫≠p tay":
    user_input = st.text_area("Nh·∫≠p vƒÉn b·∫£n", height=150)
    if user_input:
        data = [user_input]

elif input_method == "T·∫£i dataset CSV":
    uploaded_file = st.file_uploader("T·∫£i l√™n file CSV", type=["csv"])
    if uploaded_file:
        df = pd.read_csv(uploaded_file)
        cols = st.multiselect("Ch·ªçn 1 ho·∫∑c nhi·ªÅu c·ªôt ch·ª©a vƒÉn b·∫£n", df.columns)
        if cols:
            df["__combined__"] = df[cols].astype(str).agg(" ".join, axis=1)
            data = df["__combined__"].dropna().tolist()
            st.success(f"ƒê√£ k·∫øt h·ª£p {len(cols)} c·ªôt th√†nh 1 vƒÉn b·∫£n.")
            st.subheader("Xem tr∆∞·ªõc 10 d√≤ng d·ªØ li·ªáu sau khi k·∫øt h·ª£p:")
            st.dataframe(df[cols].head(10))

elif input_method == "C√†o t·ª´ web":
    url = st.text_input("üåê Nh·∫≠p URL c·∫ßn c√†o d·ªØ li·ªáu:")
    if st.button("üîç C√†o n·ªôi dung") and url:
        try:
            response = requests.get(url)
            soup = BeautifulSoup(response.text, 'html.parser')
            paragraphs = soup.find_all("p")
            data = [p.get_text() for p in paragraphs if len(p.get_text()) > 30]
            st.success(f"ƒê√£ c√†o ƒë∆∞·ª£c {len(data)} ƒëo·∫°n vƒÉn.")
            st.write(data[:10])
        except:
            st.error("Kh√¥ng th·ªÉ c√†o d·ªØ li·ªáu t·ª´ URL.")

# -----------------------------
# B∆∞·ªõc 1: TƒÉng c∆∞·ªùng d·ªØ li·ªáu
# -----------------------------
st.header("B∆∞·ªõc 1: TƒÉng c∆∞·ªùng d·ªØ li·ªáu")
option_aug = st.selectbox("Ch·ªçn ph∆∞∆°ng ph√°p tƒÉng c∆∞·ªùng", [
    "Kh√¥ng thay ƒë·ªïi", "ƒê·∫£o ng∆∞·ª£c", "Thay t·ª´ ƒë·ªìng nghƒ©a", "Thay th·∫ø th·ª±c th·ªÉ", "Th√™m nhi·ªÖu"])
aug_param = st.text_input("Nh·∫≠p tham s·ªë (n·∫øu c√≥):", help="V√≠ d·ª•: t·ª´_c≈© t·ª´_m·ªõi ho·∫∑c th·ª±c_th·ªÉ_c≈©,th·ª±c_th·ªÉ_m·ªõi")

def apply_augmentation(text):
    if option_aug == "ƒê·∫£o ng∆∞·ª£c":
        return text[::-1]
    elif option_aug == "Thay t·ª´ ƒë·ªìng nghƒ©a":
        try:
            old, new = aug_param.split()
            return text.replace(old, new)
        except:
            return text
    elif option_aug == "Thay th·∫ø th·ª±c th·ªÉ":
        try:
            old, new = aug_param.split(",")
            return text.replace(old.strip(), new.strip())
        except:
            return text
    elif option_aug == "Th√™m nhi·ªÖu":
        return "".join([char + "*" if char.isalpha() else char for char in text])
    return text

if data:
    augmented_data = [apply_augmentation(t) for t in data]
    st.subheader("K·∫øt qu·∫£ tƒÉng c∆∞·ªùng (10 d√≤ng ƒë·∫ßu):")
    st.write(augmented_data[:10])
else:
    augmented_data = []
    st.warning("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu ƒë·ªÉ tƒÉng c∆∞·ªùng.")

# -----------------------------
# B∆∞·ªõc 2: Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n
# -----------------------------
st.header("B∆∞·ªõc 2: Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n")
step2_ops = st.multiselect("Ch·ªçn c√°c thao t√°c ti·ªÅn x·ª≠ l√Ω:", [
    "T√°ch t·ª´ (Split)", "Tokenize (Spacy)", "Lo·∫°i b·ªè Stopwords", "Lo·∫°i b·ªè D·∫•u c√¢u",
    "Stemming", "Lemmatization", "POS Tagging"])

ps = PorterStemmer()
lemmatizer = WordNetLemmatizer()

def preprocess(text):
    try:
        if "Lo·∫°i b·ªè D·∫•u c√¢u" in step2_ops:
            text = text.translate(str.maketrans('', '', string.punctuation))
        tokens = text.split()
        if "Lo·∫°i b·ªè Stopwords" in step2_ops:
            stop_words = set(stopwords.words("english"))
            tokens = [t for t in tokens if t.lower() not in stop_words]
        if "Stemming" in step2_ops:
            tokens = [ps.stem(t) for t in tokens]
        if "Lemmatization" in step2_ops:
            tokens = [lemmatizer.lemmatize(t) for t in tokens]
        if "POS Tagging" in step2_ops:
            doc = nlp(" ".join(tokens))
            tokens = [f"{t.text}/{t.pos_}" for t in doc]
        if "Tokenize (Spacy)" in step2_ops:
            doc = nlp(text)
            tokens = [f"{t.text}/{t.pos_}" for t in doc]
        if "T√°ch t·ª´ (Split)" in step2_ops:
            tokens = [f'"{t}"' for t in tokens]
        return " ".join(tokens)
    except Exception as e:
        return f"[L·ªói x·ª≠ l√Ω: {e}]"

if augmented_data:
    processed_data = [preprocess(t) for t in augmented_data]
    st.subheader("K·∫øt qu·∫£ sau ti·ªÅn x·ª≠ l√Ω (10 d√≤ng ƒë·∫ßu):")
    st.write(processed_data[:10])
else:
    st.warning("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu ƒë·ªÉ x·ª≠ l√Ω.")

# --- B∆∞·ªõc 3: Vector h√≥a ---
st.header("B∆∞·ªõc 3: Vector h√≥a d·ªØ li·ªáu")
vec_option = st.selectbox("Ch·ªçn ph∆∞∆°ng ph√°p vector h√≥a", ["One-Hot Encoding", "Bag of Words"])

def one_hot_encoding(text):
    words = np.array(text.replace('"', '').split()).reshape(-1, 1)
    encoder = OneHotEncoder(sparse_output=False)
    encoded = encoder.fit_transform(words)
    return encoded

def bag_of_words(text):
    vectorizer = CountVectorizer()
    bag = vectorizer.fit_transform([text.replace('"', '')]).toarray()
    return bag, vectorizer.get_feature_names_out()

if processed_data:
    text = processed_data[0]  # l·∫•y d√≤ng ƒë·∫ßu ti√™n ƒë·ªÉ x·ª≠ l√Ω
    if vec_option == "One-Hot Encoding":
        encoded = one_hot_encoding(text)
        st.write("One-Hot Encoding Vector:")
        st.write(encoded)
    elif vec_option == "Bag of Words":
        bag, feats = bag_of_words(text)
        st.write("Bag of Words Vector:")
        st.write(f"Features: {feats}")
        st.write(bag)

else:
    st.warning("‚ö†Ô∏è Vui l√≤ng nh·∫≠p vƒÉn b·∫£n ƒë·ªÉ vector h√≥a.")

# --- B∆∞·ªõc 4: Nh√∫ng vƒÉn b·∫£n ---
st.header("B∆∞·ªõc 4: Nh√∫ng vƒÉn b·∫£n")

embed_option = st.selectbox(
    "Ch·ªçn ph∆∞∆°ng ph√°p nh√∫ng", 
    ["Bag of N-grams", "TF-IDF", "FastText", "GloVe", "BERT", "GPT-2"]
)

# FastText
@st.cache_data
def load_fasttext():
    fasttext.util.download_model('en', if_exists='ignore')
    return fasttext.load_model('cc.en.300.bin')

# GloVe
@st.cache_data
def load_glove():
    return gensim_load("glove-wiki-gigaword-50")

# BERT
from transformers import BertTokenizer, BertModel, GPT2Tokenizer, GPT2Model
import torch

@st.cache_resource
def load_bert():
    tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
    model = BertModel.from_pretrained("bert-base-uncased")
    return tokenizer, model

# GPT-2
@st.cache_resource
def load_gpt2():
    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    model = GPT2Model.from_pretrained("gpt2")
    return tokenizer, model

# ===== C√°c h√†m x·ª≠ l√Ω nh√∫ng =====

def bag_of_ngrams(text, n=2):
    vectorizer = CountVectorizer(ngram_range=(n, n))
    bag = vectorizer.fit_transform([text.replace('"', '')]).toarray()
    return bag, vectorizer.get_feature_names_out()

def tfidf_vectorization(text):
    vectorizer = TfidfVectorizer()
    tfidf = vectorizer.fit_transform([text.replace('"', '')]).toarray()
    return tfidf, vectorizer.get_feature_names_out()

def fasttext_embedding(text):
    ft = load_fasttext()
    words = text.replace('"', '').split()
    embeddings = [ft.get_word_vector(word) for word in words]
    return np.array(embeddings)

def glove_embedding(text):
    model = load_glove()
    words = text.replace('"', '').split()
    embeddings = [model[word] for word in words if word in model]
    return np.array(embeddings)

def bert_embedding(text):
    tokenizer, model = load_bert()
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1).numpy()

def gpt2_embedding(text):
    tokenizer, model = load_gpt2()
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1).numpy()

# ===== Th·ª±c hi·ªán nh√∫ng n·∫øu c√≥ d·ªØ li·ªáu =====

if processed_data:
    text = processed_data[0]  # L·∫•y d√≤ng ƒë·∫ßu ti√™n ƒë·ªÉ th·ª≠ nghi·ªám nh√∫ng
    if embed_option == "Bag of N-grams":
        bag, feats = bag_of_ngrams(text)
        st.write("Bag of N-grams:")
        st.write(f"Features: {feats}")
        st.write(bag)
    elif embed_option == "TF-IDF":
        tfidf, feats = tfidf_vectorization(text)
        st.write("TF-IDF:")
        st.write(f"Features: {feats}")
        st.write(tfidf)
    elif embed_option == "FastText":
        emb = fasttext_embedding(text)
        st.write("FastText Embedding:")
        st.write(emb)
    elif embed_option == "GloVe":
        emb = glove_embedding(text)
        st.write("GloVe Embedding:")
        st.write(emb)
    elif embed_option == "BERT":
        emb = bert_embedding(text)
        st.write("BERT Embedding:")
        st.write(emb)
    elif embed_option == "GPT-2":
        emb = gpt2_embedding(text)
        st.write("GPT-2 Embedding:")
        st.write(emb)
else:
    st.warning("‚ö†Ô∏è Vui l√≤ng nh·∫≠p v√† x·ª≠ l√Ω vƒÉn b·∫£n tr∆∞·ªõc khi nh√∫ng.")

st.header("B∆∞·ªõc 5: Hu·∫•n luy·ªán v√† D·ª± ƒëo√°n m√¥ h√¨nh")

uploaded_file_5 = st.file_uploader("T·∫£i file CSV d√πng cho hu·∫•n luy·ªán (ri√™ng B∆∞·ªõc 5)", type=["csv"], key="step5_upload")

if uploaded_file_5:
    try:
        try:
            df_5 = pd.read_csv(uploaded_file_5)
        except UnicodeDecodeError:
            df_5 = pd.read_csv(uploaded_file_5, encoding="ISO-8859-1")

        if df_5.empty:
            st.error("File CSV tr·ªëng. Vui l√≤ng ki·ªÉm tra l·∫°i.")
        else:
            st.success(f"ƒê√£ n·∫°p {df_5.shape[0]} d√≤ng, {df_5.shape[1]} c·ªôt.")
            st.dataframe(df_5.head())

            label_col = st.selectbox("Ch·ªçn c·ªôt nh√£n (label)", df_5.columns, key="step5_label")
            text_col = st.selectbox("Ch·ªçn c·ªôt ch·ª©a vƒÉn b·∫£n", df_5.columns.drop(label_col), key="step5_text")

            if label_col and text_col:
                X = df_5[text_col].astype(str).tolist()
                y = df_5[label_col].astype(str).tolist()

                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

                vec_type = st.selectbox("Vectorizer", ["CountVectorizer", "TF-IDF"], key="step5_vectorizer")
                vectorizer = CountVectorizer() if vec_type == "CountVectorizer" else TfidfVectorizer()
                X_train_vec = vectorizer.fit_transform(X_train)
                X_test_vec = vectorizer.transform(X_test)

                model_name = st.selectbox("M√¥ h√¨nh", ["Naive Bayes", "Logistic Regression", "Decision Tree", "KNN"], key="step5_model")

                if model_name == "Naive Bayes":
                    model = MultinomialNB()
                elif model_name == "Logistic Regression":
                    model = LogisticRegression(max_iter=500)
                elif model_name == "Decision Tree":
                    model = DecisionTreeClassifier(max_depth=5)
                elif model_name == "KNN":
                    model = KNeighborsClassifier(n_neighbors=5)

                if st.button("Hu·∫•n luy·ªán m√¥ h√¨nh", key="step5_train"):
                    model.fit(X_train_vec, y_train)
                    st.session_state['model'] = model
                    st.session_state['vectorizer'] = vectorizer

                    predictions = model.predict(X_test_vec)
                    acc = metrics.accuracy_score(y_test, predictions)

                    st.success(f"ƒê√£ hu·∫•n luy·ªán m√¥ h√¨nh {model_name}")
                    st.markdown(f"**üéØ ƒê·ªô ch√≠nh x√°c:** `{acc:.2%}`")

                    st.markdown("**üìã B√°o c√°o ph√¢n lo·∫°i (r√∫t g·ªçn):**")
                    report = metrics.classification_report(y_test, predictions, output_dict=True)
                    report_df = pd.DataFrame(report).transpose().round(2).iloc[:-1]
                    st.dataframe(report_df)

                    # MA TR·∫¨N NH·∫¶M L·∫™N (BI·ªÇU ƒê·ªí DUY NH·∫§T)
                    st.markdown("**Ma tr·∫≠n nh·∫ßm l·∫´n (bi·ªÉu ƒë·ªì):**")
                    cm = metrics.confusion_matrix(y_test, predictions)
                    labels = sorted(list(set(y_test)))
                    labels = [str(label) for label in labels]

                    fig_cm, ax_cm = plt.subplots(figsize=(6, 4))
                    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                                xticklabels=labels, yticklabels=labels, ax=ax_cm)
                    ax_cm.set_xlabel("Nh√£n d·ª± ƒëo√°n")
                    ax_cm.set_ylabel("Nh√£n th·ª±c t·∫ø")
                    ax_cm.set_title(f"Confusion Matrix - {model_name}")
                    st.pyplot(fig_cm)

                    # BI·ªÇU ƒê·ªí PRECISION / RECALL / F1
                    st.markdown("**Bi·ªÉu ƒë·ªì Precision / Recall / F1-score:**")
                    fig_metrics, ax_metrics = plt.subplots(figsize=(8, 4))
                    report_df[['precision', 'recall', 'f1-score']].plot(kind='bar', ax=ax_metrics)
                    ax_metrics.set_ylim(0, 1)
                    ax_metrics.set_title(f"Ch·ªâ s·ªë theo l·ªõp - {model_name}")
                    ax_metrics.set_ylabel("Gi√° tr·ªã")
                    plt.xticks(rotation=45)
                    st.pyplot(fig_metrics)

            # D·ª∞ ƒêO√ÅN VƒÇN B·∫¢N M·ªöI
            user_input = st.text_input("Nh·∫≠p vƒÉn b·∫£n ƒë·ªÉ m√¥ h√¨nh d·ª± ƒëo√°n", key="step5_predict")
            if st.button("üîç D·ª± ƒëo√°n vƒÉn b·∫£n", key="step5_runpredict") and user_input:
                if 'model' in st.session_state and 'vectorizer' in st.session_state:
                    try:
                        transformed = st.session_state['vectorizer'].transform([user_input])
                        prediction = st.session_state['model'].predict(transformed)[0]
                        st.success(f"üîÆ D·ª± ƒëo√°n: **{prediction}**")
                    except Exception as e:
                        st.error(f"L·ªói khi d·ª± ƒëo√°n: {e}")
                else:
                    st.warning("‚ö†Ô∏è C·∫ßn hu·∫•n luy·ªán m√¥ h√¨nh tr∆∞·ªõc khi d·ª± ƒëo√°n.")

    except Exception as e:
        st.error(f"L·ªói khi ƒë·ªçc file: {e}")
else:
    st.warning("‚ö†Ô∏è Vui l√≤ng ch·ªçn file CSV h·ª£p l·ªá ƒë·ªÉ hu·∫•n luy·ªán.")

st.header("B∆∞·ªõc 6: H·ªá th·ªëng Recommedation")

# T·∫£i 2 file: 1 cho ratings, 1 cho movies
ratings_file = st.file_uploader("T·∫£i file ratings.csv", type=["csv"], key="step6_ratings")
movies_file = st.file_uploader("T·∫£i file movies.csv", type=["csv"], key="step6_movies")

@st.cache_data
def read_csv_file(uploaded_file):
    try:
        return pd.read_csv(uploaded_file)
    except UnicodeDecodeError:
        return pd.read_csv(uploaded_file, encoding="ISO-8859-1")

@st.cache_data
def train_svd(ratings_pivot, k=50):
    U, S, Vt = np.linalg.svd(ratings_pivot, full_matrices=False)
    S_k = np.diag(S[:k])
    U_k = U[:, :k]
    Vt_k = Vt[:k, :]
    return U_k, S_k, Vt_k

def compute_v_new(Vt_k, movie_idx):
    return Vt_k[:, movie_idx]

def recommend_movies(movie_name, ratings_pivot, Vt_k, movies, top_n=5):
    movie_id = movies[movies['title'] == movie_name]['movieId']
    if movie_id.empty:
        return None, "Kh√¥ng t√¨m th·∫•y phim n√†y!"

    movie_id_value = movie_id.values[0]
    if movie_id_value not in ratings_pivot.index:
        return None, "Phim ƒë√£ ƒë∆∞·ª£c n·∫°p nh∆∞ng ch∆∞a c√≥ rating."

    movie_idx = ratings_pivot.index.get_loc(movie_id_value)
    V_new = compute_v_new(Vt_k, movie_idx)
    similarity_scores = cosine_similarity(Vt_k.T, V_new.reshape(1, -1))[:, 0]

    similar_indices = np.argsort(-similarity_scores)[1:top_n+1]
    valid_indices = [i for i in similar_indices if i < len(ratings_pivot)]
    recommended_ids = ratings_pivot.index[valid_indices].tolist()
    recommended_movies = movies[movies['movieId'].isin(recommended_ids)]
    return recommended_movies, None

if ratings_file and movies_file:
    ratings = read_csv_file(ratings_file)
    movies = read_csv_file(movies_file)

    if ratings.empty or movies.empty:
        st.error("‚ö†Ô∏è File r·ªóng ho·∫∑c kh√¥ng ƒë·ªçc ƒë∆∞·ª£c.")
    else:
        max_samples = st.slider("S·ªë l∆∞·ª£ng ratings t·ªëi ƒëa", 1000, 100000, 5000, step=1000)
        if len(ratings) > max_samples:
            ratings = ratings.sample(n=max_samples, random_state=42)

        ratings_pivot = ratings.pivot(index='movieId', columns='userId', values='rating').fillna(0)
        U_k, S_k, Vt_k = train_svd(ratings_pivot)

        selected_movie = st.text_input("üîç Nh·∫≠p t√™n phim:", "")
        if st.button("üéÆ G·ª£i √Ω phim") and selected_movie:
            if selected_movie in movies['title'].values:
                movie_details = movies[movies['title'] == selected_movie].iloc[0]
                st.subheader(f"Th√¥ng tin phim: {selected_movie}")
                st.write(f"üé≠ **Th·ªÉ lo·∫°i:** {movie_details['genres']}")
                recommendations, error_message = recommend_movies(selected_movie, ratings_pivot, Vt_k, movies)

                if isinstance(recommendations, pd.DataFrame) and not recommendations.empty:
                    st.subheader("üé¨ Phim g·ª£i √Ω:")
                    for _, row in recommendations.iterrows():
                        st.markdown(f"üé• **{row['title']}**  üé≠ {row['genres']}")
                elif error_message:
                    st.warning(error_message)
            else:
                st.error("T√™n phim kh√¥ng t·ªìn t·∫°i trong danh s√°ch.")
else:
    st.info("Vui l√≤ng t·∫£i ƒë·ªß 2 file: `ratings.csv` v√† `movies.csv`.")
    

st.header("B∆∞·ªõc 7: Chatbot")

import unicodedata
from sklearn.pipeline import make_pipeline

# H√†m x·ª≠ l√Ω ti·∫øng Vi·ªát kh√¥ng d·∫•u
def remove_accents(text):
    return ''.join(c for c in unicodedata.normalize('NFD', text)
                   if unicodedata.category(c) != 'Mn').lower()

try:
    df_players = pd.read_csv("attacking.csv")
    df_intents = pd.read_csv("intents.csv")

    model_chat = make_pipeline(TfidfVectorizer(), MultinomialNB())
    model_chat.fit(df_intents['question'], df_intents['intent'])

    # Giao di·ªán h·ªôi tho·∫°i
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    def chatbot_response(user_input):
        predicted_intent = model_chat.predict([user_input])[0]
        normalized_input = remove_accents(user_input)

        for _, row in df_players.iterrows():
            player_norm = remove_accents(row['player_name'])
            if player_norm in normalized_input:
                value = row[predicted_intent]
                if predicted_intent == "assists":
                    return f"{row['player_name']} ƒë√£ ki·∫øn t·∫°o {value} l·∫ßn."
                elif predicted_intent == "match_played":
                    return f"{row['player_name']} ƒë√£ thi ƒë·∫•u {value} tr·∫≠n."
                elif predicted_intent == "dribbles":
                    return f"{row['player_name']} ƒë√£ r√™ b√≥ng {value} l·∫ßn."
                elif predicted_intent == "corner_taken":
                    return f"{row['player_name']} ƒë√£ ƒë√° {value} qu·∫£ ph·∫°t g√≥c."
                elif predicted_intent == "offsides":
                    return f"{row['player_name']} ƒë√£ vi·ªát v·ªã {value} l·∫ßn."
                elif predicted_intent == "position":
                    return f"{row['player_name']} ch∆°i ·ªü v·ªã tr√≠ {value}."
                elif predicted_intent == "club":
                    return f"{row['player_name']} thu·ªôc c√¢u l·∫°c b·ªô {value}."
        return "‚ùì T√¥i kh√¥ng hi·ªÉu. H√£y h·ªèi nh∆∞: 'Ronaldo ƒë√° v·ªã tr√≠ n√†o?' ho·∫∑c 'Messi r√™ b√≥ng bao nhi√™u l·∫ßn?'"

    with st.form("chat_form"):
        user_input = st.text_input("B·∫°n", placeholder="Nh·∫≠p c√¢u h·ªèi v·ªÅ c·∫ßu th·ªß...")
        submitted = st.form_submit_button("G·ª≠i")

    if submitted and user_input:
        response = chatbot_response(user_input)
        st.session_state.chat_history.append((user_input, response))

    for user, bot in reversed(st.session_state.chat_history):
        st.markdown(f"<div style='background-color:#d1ecf1; padding:8px; border-radius:5px; margin:4px'><b>B·∫°n:</b> {user}</div>", unsafe_allow_html=True)
        st.markdown(f"<div style='background-color:#fff3cd; padding:8px; border-radius:5px; margin:4px'><b>Bot:</b> {bot}</div>", unsafe_allow_html=True)

except Exception as e:
    st.error(f"‚ö†Ô∏è Kh√¥ng th·ªÉ load chatbot: {e}")